{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from numpy import short\n",
    "sys.path.append( '/home/PJLAB/chenyun/trade_test/DI-engine')\n",
    "print(sys.path)\n",
    "from typing import Union, Optional, List, Any, Tuple\n",
    "import os\n",
    "import torch\n",
    "from ditk import logging\n",
    "from functools import partial\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from ding.envs import get_vec_env_setting, create_env_manager\n",
    "from ding.worker import BaseLearner, InteractionSerialEvaluator, BaseSerialCommander, create_buffer, \\\n",
    "    create_serial_collector\n",
    "from ding.config import read_config, compile_config\n",
    "from ding.policy import create_policy\n",
    "from ding.utils import set_pkg_seed\n",
    "from ding.entry.utils import random_collect\n",
    "from easydict import EasyDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep = 1\n",
    "stocks_dqn_config = dict(\n",
    "    exp_name='stocks_test_v8',\n",
    "    env=dict(\n",
    "        # Whether to use shared memory. Only effective if \"env_manager_type\" is 'subprocess'\n",
    "        # Env number respectively for collector and evaluator.\n",
    "        collector_env_num=8,\n",
    "        evaluator_env_num=8,\n",
    "        env_id='stocks-v0',\n",
    "        n_evaluator_episode=8,\n",
    "        stop_value=500,\n",
    "        eps_length = 253,\n",
    "        window_size = 20,\n",
    "    ),\n",
    "    policy=dict(\n",
    "        # Whether to use cuda for network.\n",
    "        cuda=True,\n",
    "        model=dict(\n",
    "            obs_shape= 61,\n",
    "            action_shape=5,\n",
    "            encoder_hidden_size_list=[128],\n",
    "            head_layer_num = 1,\n",
    "            # Whether to use dueling head.\n",
    "            dueling=True,\n",
    "        ),\n",
    "        # Reward's future discount factor, aka. gamma.\n",
    "        discount_factor=0.99,\n",
    "        # How many steps in td error.\n",
    "        nstep=nstep,\n",
    "        # learn_mode config\n",
    "        learn=dict(\n",
    "            update_per_collect=10,\n",
    "            batch_size=64,\n",
    "            learning_rate=0.001,\n",
    "            # Frequency of target network update.\n",
    "            target_update_freq=100,\n",
    "        ),\n",
    "        # collect_mode config\n",
    "        collect=dict(\n",
    "            # You can use either \"n_sample\" or \"n_episode\" in collector.collect.\n",
    "            # Get \"n_sample\" samples per collect.\n",
    "            n_sample=64,\n",
    "            # Cut trajectories into pieces with length \"unroll_len\".\n",
    "            unroll_len=1,\n",
    "        ),\n",
    "        # command_mode config\n",
    "        other=dict(\n",
    "            # Epsilon greedy with decay.\n",
    "            eps=dict(\n",
    "                # Decay type. Support ['exp', 'linear'].\n",
    "                type='exp',\n",
    "                start=0.95,\n",
    "                end=0.1,\n",
    "                decay=50000,\n",
    "            ),\n",
    "            replay_buffer=dict(replay_buffer_size=100000, )\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "stocks_dqn_config = EasyDict(stocks_dqn_config)\n",
    "main_config = stocks_dqn_config\n",
    "\n",
    "stocks_dqn_create_config = dict(\n",
    "    env=dict(\n",
    "        type='stocks-v0',\n",
    "        import_names=['dizoo.trading_test.envs.stocks_env'],\n",
    "    ),\n",
    "    # env_manager=dict(type='subprocess'),\n",
    "    env_manager=dict(type='base'),\n",
    "    policy=dict(type='dqn'),\n",
    ")\n",
    "stocks_dqn_create_config = EasyDict(stocks_dqn_create_config)\n",
    "create_config = stocks_dqn_create_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_pipeline(\n",
    "        input_cfg: Union[str, Tuple[dict, dict]],\n",
    "        seed: int = 0,\n",
    "        env_setting: Optional[List[Any]] = None,\n",
    "        model: Optional[torch.nn.Module] = None,\n",
    "        max_train_iter: Optional[int] = int(1e7),\n",
    "        max_env_step: Optional[int] = int(1e7),\n",
    ") -> 'Policy':  # noqa\n",
    "    \"\"\"\n",
    "    Overview:\n",
    "        Serial pipeline entry for off-policy RL.\n",
    "    Arguments:\n",
    "        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type. \\\n",
    "            ``str`` type means config file path. \\\n",
    "            ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n",
    "        - seed (:obj:`int`): Random seed.\n",
    "        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements: \\\n",
    "            ``BaseEnv`` subclass, collector env config, and evaluator env config.\n",
    "        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\n",
    "        - max_train_iter (:obj:`Optional[int]`): Maximum policy update iterations in training.\n",
    "        - max_env_step (:obj:`Optional[int]`): Maximum collected environment interaction steps.\n",
    "    Returns:\n",
    "        - policy (:obj:`Policy`): Converged policy.\n",
    "    \"\"\"\n",
    "    if isinstance(input_cfg, str):\n",
    "        cfg, create_cfg = read_config(input_cfg)\n",
    "    else:\n",
    "        cfg, create_cfg = input_cfg\n",
    "    create_cfg.policy.type = create_cfg.policy.type + '_command'\n",
    "    env_fn = None if env_setting is None else env_setting[0]\n",
    "    cfg = compile_config(cfg, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True)\n",
    "    print(json.dumps(cfg, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "    print(\"config compiling complate!\")\n",
    "\n",
    "\n",
    "    # Create main components: env, policy\n",
    "    if env_setting is None:\n",
    "        env_fn, collector_env_cfg, evaluator_env_cfg = get_vec_env_setting(cfg.env)\n",
    "    else:\n",
    "        env_fn, collector_env_cfg, evaluator_env_cfg = env_setting\n",
    "    \n",
    "\n",
    "    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n",
    "    evaluator_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in evaluator_env_cfg])\n",
    "\n",
    "    \n",
    "    collector_env.seed(cfg.seed)\n",
    "    evaluator_env.seed(cfg.seed, dynamic_seed=False)\n",
    "    set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)\n",
    "    policy = create_policy(cfg.policy, model=model, enable_field=['learn', 'collect', 'eval', 'command'])\n",
    "\n",
    "    # Create worker components: learner, collector, evaluator, replay buffer, commander.\n",
    "    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n",
    "    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name)\n",
    "    collector = create_serial_collector(\n",
    "        cfg.policy.collect.collector,\n",
    "        env=collector_env,\n",
    "        policy=policy.collect_mode,\n",
    "        tb_logger=tb_logger,\n",
    "        exp_name=cfg.exp_name\n",
    "    )\n",
    "    evaluator = InteractionSerialEvaluator(\n",
    "        cfg.policy.eval.evaluator, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name\n",
    "    )\n",
    "    replay_buffer = create_buffer(cfg.policy.other.replay_buffer, tb_logger=tb_logger, exp_name=cfg.exp_name)\n",
    "    commander = BaseSerialCommander(\n",
    "        cfg.policy.other.commander, learner, collector, evaluator, replay_buffer, policy.command_mode\n",
    "    )\n",
    "\n",
    "    # ==========\n",
    "    # Main loop\n",
    "    # ==========\n",
    "    # Learner's before_run hook.\n",
    "    learner.call_hook('before_run')\n",
    "\n",
    "    # Accumulate plenty of data at the beginning of training.\n",
    "    if cfg.policy.get('random_collect_size', 0) > 0:\n",
    "        random_collect(cfg.policy, policy, collector, collector_env, commander, replay_buffer)\n",
    "    while True:\n",
    "        collect_kwargs = commander.step()\n",
    "        # Evaluate policy performance\n",
    "        if evaluator.should_eval(learner.train_iter):\n",
    "            print(\"+++++++++++++++++++++++++++++++++++++++++++++++++START EVAL!\")\n",
    "            stop, reward, cy_info = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)\n",
    "            print(\"+++++++++++++++++++++++++++++++++++++++++++++++++END EVAL!\")\n",
    "            global shortnum\n",
    "            global longnum\n",
    "            global flatnum\n",
    "            shortnum += cy_info[0]\n",
    "            longnum += cy_info[1]\n",
    "            flatnum += cy_info[2]\n",
    "            print(\"short:\",shortnum)\n",
    "            print(\"long:\",longnum)\n",
    "            print(\"flat:\",flatnum)\n",
    "            \n",
    "            if stop:\n",
    "                break\n",
    "        # Collect data by default config n_sample/n_episode\n",
    "        new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs=collect_kwargs)\n",
    "        replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)\n",
    "        # Learn policy from collected data\n",
    "        for i in range(cfg.policy.learn.update_per_collect):\n",
    "            # Learner will train ``update_per_collect`` times in one iteration.\n",
    "            train_data = replay_buffer.sample(learner.policy.get_attribute('batch_size'), learner.train_iter)\n",
    "            if train_data is None:\n",
    "                # It is possible that replay buffer's data count is too few to train ``update_per_collect`` times\n",
    "                logging(\n",
    "                    \"Replay buffer's data can only train for {} steps. \".format(i) +\n",
    "                    \"You can modify data collect config, e.g. increasing n_sample, n_episode.\"\n",
    "                )\n",
    "                break\n",
    "            learner.train(train_data, collector.envstep)\n",
    "            if learner.policy.get_attribute('priority'):\n",
    "                replay_buffer.update(learner.priority_info)\n",
    "        if collector.envstep >= max_env_step or learner.train_iter >= max_train_iter:\n",
    "            break\n",
    "\n",
    "    # Learner's after_run hook.\n",
    "    learner.call_hook('after_run')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortnum = 0\n",
    "longnum = 0\n",
    "flatnum = 0\n",
    "serial_pipeline([main_config, create_config], seed=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cloud_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90b679173d10dba7613fe73d27e4565710a2f8c3d3409afafc0875c2e81d765b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
